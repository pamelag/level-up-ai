{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd9SBlu9RvM1LAep/JHgQA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pamelag/level-up-ai/blob/main/LLM_Expr7_101(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0jrf1dKz-JW"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "HU8Kmwly0XD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "yjZV3-_80Zg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "jX24mXhA0b5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The most important person in AI is\"\n",
        "encoding = tokenizer(text, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "MEeVouOv0eVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 128\n",
        "generation_config.repetition_penalty = 1.18\n",
        "generation_config.temperature = 0.0000001"
      ],
      "metadata": {
        "id": "9yWG0ScQ0ims"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 128\n",
        "generation_config.repetition_penalty = 1.18\n",
        "generation_config.temperature = 0.0000001"
      ],
      "metadata": {
        "id": "bORHMi020nDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "2DDd_wo00qgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = encoding.to(model.device)"
      ],
      "metadata": {
        "id": "b-Mk1Lua0vqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )"
      ],
      "metadata": {
        "id": "75UTGuaw0wxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "7YlKwsMU03i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
      ],
      "metadata": {
        "id": "t9Zu4_fx04Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config.max_new_tokens = 512\n",
        "generation_config.repetition_penalty = 1.18\n",
        "generation_config.temperature = 0.0000001"
      ],
      "metadata": {
        "id": "JCfvHxnG063r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer, pipeline"
      ],
      "metadata": {
        "id": "bZJ_VFFL1oiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "xHVszZBq1rof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    generation_config=generation_config,\n",
        "    num_return_sequences=1,\n",
        "    streamer=streamer,\n",
        ")"
      ],
      "metadata": {
        "id": "bD05Q4Lf1uow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\"Who is the most important person in AI?\")"
      ],
      "metadata": {
        "id": "GHiG4fa81xHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a professional writer. Rewrite the following text in simple words.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ka4xEC4O1zYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''Strategy & Innovation team is working on innovative intelligent document processing capabilities as part of Implement Intelligent Automation Solutions Aspire Initiative under Strong,\n",
        "Secure Digital Foundation strategic Program and Embrace Innovation Focus Area. The initiative lead of implement intelligent automation solutions is Jocelyn Yem, Director, Strategy & Innovation.\n",
        "High Level Action for this initiative is to Implement innovative intelligent document processing capabilities. Major Milestones - Completed Proof of Value work and select Proof of Concept tool(s),\n",
        "Expanded intelligent document processing capability across Revenue NSW (inc. Luminance). Developed scaled Proof of Concept'''\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Analyze the text and write the main points.\n",
        "```\n",
        "{text}\n",
        "```\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "oS8V9V-k2Db-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a professional writer. Rewrite the following text in formal words.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VYnl8ScI2Fk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''Employee Voice is an Aspire Initiative under Strong, Future Fit Workforce strategic Program and People at the Heart Strategic Focus Area. The initiative lead of strategic workforce plan solutions is\n",
        "    Scott Johnston, Deputy Secretary, Revenue NSW. The High Level actions are to Derive insights from the organisational People Matters Engagement Survey (PMES) results & implement an action plan to focus on areas for opportunity\n",
        "    Major Milestones - Delivered focused PMES communication plan, Developed PMES action plans in each business unit and across Revenue NSW,\n",
        "    Implemented Revenue NSW PMES action plans, using existing feedback mechanisms where possible to evaluate action.'''\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Analyze the text and write the main points.\n",
        "```\n",
        "{text}\n",
        "```\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "dRojeg7L2YF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "\n",
        "def predict(prompt: str, system_prompt: Optional[str] = None):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        }\n",
        "    ]\n",
        "    if system_prompt:\n",
        "        messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    return llm(prompt)"
      ],
      "metadata": {
        "id": "rQdHI5sJ2bXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "output = predict(prompt, system_prompt)"
      ],
      "metadata": {
        "id": "Uxr_LtSw2ccA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"*/models/meta-llama/Llama-2-7b-chat-hf\")"
      ],
      "metadata": {
        "id": "AKReQUOb2fSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Faster Inference"
      ],
      "metadata": {
        "id": "mbGGL1AJ2kTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16\n",
        "# )\n",
        "\n",
        "# generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "# generation_config.max_new_tokens = 512\n",
        "# generation_config.repetition_penalty = 1.18\n",
        "# generation_config.temperature = 0.0000001"
      ],
      "metadata": {
        "id": "WaaNKW7V2hab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "_cNhUCEN2vhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True,\n",
        "    load_in_8bit=True,\n",
        ")"
      ],
      "metadata": {
        "id": "-ncfI2UG2yC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6F8yerKA20s1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}